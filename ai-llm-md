# ğŸŒŒ CosmosWatcher â€“ Unified Agent and Backend Architecture Prompt

**Version:** July 2025 (Updated)
**Components:** Observer (agent), Singularity (backend), Horizon (frontend)
**Focus:** Observer design, secure registration, metrics ingestion, scalable storage using TimescaleDB, and action orchestration via Ping

---

## ğŸ§  Overview

CosmosWatcher is a multi-tenant system observation and control platform. It consists of:

* **Observer**: A Go-based agent that collects and sends system metrics.
* **Singularity**: A Go server (Gorilla Mux + PostgreSQL + TimescaleDB) that handles secure registration, telemetry, and action coordination.
* **Horizon**: A SvelteKit-based frontend for viewing agents, metrics, and actions.

---

## ğŸ“¦ Database Functions

Database functions for Singularity are located in `main/internal/database`.
Usage:

```go
import db "main/internal/database"

func UpdateObserverHeartbeat(observerID string, tenantID uuid.UUID, seenAt time.Time) error {
	_, err := dbs.Exec(`
		UPDATE observer
		SET last_seen = $1
		WHERE id = $2 AND tenant_id = $3
	`, seenAt, observerID, tenantID)
	return err
}
```

## ğŸš€ Observer: Agent Design

### Core Responsibilities

* Secure bootstrap using a one-time token
* Send periodic heartbeat pings
* Collect system metrics via modular collectors
* Support remote instructions (actions)
* Optional: Auto-update and run as a service

### ğŸŒŸ Goal

Enable efficient collection, storage, and querying of time-series metrics (e.g., CPU, disk, memory, temperature, network) from 10,000â€“1,000,000+ Observer agents.

### ğŸ—‚ï¸ Observer Project Structure

```
observer/
â”œâ”€â”€ cmd/           # CLI commands (install, run, update)
â”œâ”€â”€ collector/     # Modular collectors (e.g., cpu.go, disk.go, mem.go, net.go, temp.go)
â”œâ”€â”€ internal/
â”‚   â”œâ”€â”€ bootstrap/ # Bootstrap logic
â”‚   â”œâ”€â”€ ping/      # Ping logic
â”‚   â”œâ”€â”€ config/    # YAML config via Viper
â”‚   â”œâ”€â”€ queue/     # Metric batching and flushing
â”‚   â””â”€â”€ service/   # Service install/run logic
â”œâ”€â”€ types/         # Shared metric struct
â””â”€â”€ observer.yaml  # Config file with tokens and IDs
```

### ğŸ“ observer.yaml Format

```yaml
bootstrap_token: abc123             # Used once for registration
observer_token: f36b...             # Persistent token used after bootstrap
server_url: https://api.myorg.com
observer_id: 123e...
tenant_id: 456a...
```

## ğŸ” Bootstrap and Authentication

### Bootstrap Flow

Observer POSTs to /api/observer/bootstrap with Authorization: Bearer \<bootstrap\_token>

Server returns:

```json
{
  "observer_id": "...",
  "tenant_id": "...",
  "observer_token": "..."
}
```

Observer stores this in observer.yaml

### Auth Flow

* /observer/bootstrap â†’ requires bootstrap token
* All other /observer/... routes â†’ require persistent observer\_token

Tokens are stored in:

* `observer_bootstrap_token` table
* `observer_tokens` table

## ğŸ“¡ Ping Handler & Actions

### Purpose of /api/observer/ping

* Acts as a heartbeat signal
* Confirms observer is alive
* Optionally sends minimal metrics
* Returns optional action payload

### Example Ping Flow

Observer sends:

```json
{
  "timestamp": "2025-07-14T20:00:00Z",
  "metrics": {
    "cpu": 12.5,
    "mem_free": 2048
  }
}
```

Server replies:

```json
{
  "success": true,
  "next_interval": 30,
  "action": {
    "type": "collect-once",
    "target": "disk",
    "args": { "path": "/var/log" }
  }
}
```

## ğŸ”„ Action Handling Plan

### Supported Action Types

* `collect-once`: Run a one-time collector immediately
* `update-now`: Force an agent update
* `shutdown`: Gracefully stop the agent

### Dispatch Design

```go
if res.Action != nil {
	log.Printf("Action received: %s on %s", res.Action.Type, res.Action.Target)
	dispatcher.Dispatch(res.Action)
}
```

* No retries or queues â€” one action per ping.

## ğŸ“¦ Metrics Architecture

### Backend: TimescaleDB

CosmosWatcher uses TimescaleDB with hypertables for high-ingestion time-series workloads.

### Why Timescale?

* Built on PostgreSQL
* Native time-based partitioning
* Compression & retention policies
* High insert throughput

### Table Design

```sql
CREATE TABLE observer_metrics (
  time         TIMESTAMPTZ      NOT NULL,
  observer_id  UUID             NOT NULL,
  tenant_id    UUID             NOT NULL,
  metric_name  TEXT             NOT NULL,
  value        DOUBLE PRECISION NOT NULL,
  labels       JSONB            DEFAULT '{}'::JSONB
);

SELECT create_hypertable('observer_metrics', 'time');

ALTER TABLE observer_metrics SET (
  timescaledb.compress,
  timescaledb.compress_orderby = 'time DESC',
  timescaledb.compress_segmentby = 'observer_id'
);

SELECT add_compression_policy('observer_metrics', INTERVAL '7 days');
```

## â±ï¸ Collector Runtime Design

### Core Loops

```go
startPingLoop(conf)      // Sends heartbeat and receives actions
startCollectors(conf)    // Starts collectors (CPU, mem, etc.)
flush.StartFlusher(conf) // Posts metrics to /observer/data every 15s
```

### Collectors Implemented

| Metric Name         | Collector | Interval | Notes                                 |
| ------------------- | --------- | -------- | ------------------------------------- |
| cpu\_usage          | cpu.go    | 10s      | Aggregate, 600ms sample               |
| mem\_total          | mem.go    | 10s      | MB                                    |
| mem\_used           | mem.go    | 10s      | MB                                    |
| mem\_free           | mem.go    | 10s      | MB (uses Available)                   |
| disk\_used\_percent | disk.go   | 60s      | Per mount, label: {"mount": "/"}      |
| net\_bytes\_sent    | net.go    | 30s      | Per interface                         |
| net\_bytes\_recv    | net.go    | 30s      | Per interface                         |
| temp\_celsius       | temp.go   | 60s      | Per sensor, skips unsupported systems |

## ğŸ”„ Summary of Metric Flow

```
[Observer]
  |
  |---> /observer/ping     â†’ heartbeat, light metrics, returns actions
  |---> /observer/data     â†’ bulk metric ingest
                         â†’ JSON: {metrics: [{time, metric_name, value, labels}]}
  |
[Singularity]
  |
  |---> TimescaleDB (hypertable: observer_metrics)
  |
  |---> Optional: events, alerts, dashboard summaries
```

## ğŸ“ˆ Scaling Plan

* TimescaleDB hypertables + compression
* Each metric entry â‰ˆ 200 bytes
* 100,000 agents \* 6 metrics/min â‰ˆ 120 MB/hour uncompressed
* Scales via chunking, compression, read replicas
* `/observer/data` is separate from pings for efficient batch ingestion

## âœ… Completed Steps

* âœ… Persistent token stored and loaded from observer.yaml
* âœ… Ping endpoint and action response working
* âœ… TimescaleDB schema with hypertable and compression
* âœ… DB insert function: InsertObserverMetrics
* âœ… /observer/data POST endpoint for bulk metrics
* âœ… Thread-safe queue + 15s flusher
* âœ… Collectors implemented:

  * CPU
  * Memory
  * Disk
  * Network
  * Temp (conditional)
* âœ… Observer dashboard implemented in Horizon frontend:

  * Observer list grouped per tenant
  * Metrics shown per observer: CPU, Memory, Disk, Network, Temp
  * Supports `last_seen` and `last_uptime_seconds`
  * Metrics fetched from TimescaleDB using `latest_metrics` CTE
  * Observers without metrics are omitted
  * Sorted by name client-side

## ğŸ”„ Optional Next Steps

* Implement dispatcher.Dispatch for collect-once actions
* Add observer\_status table for alerts/summaries
* Support config sync from backend
* Add auto-update support to Observer
* Build graph views and per-observer drilldowns in Horizon
* Add metric overlays and time-based filters
